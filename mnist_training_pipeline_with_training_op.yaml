apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: mnist-training-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
    pipelines.kubeflow.org/pipeline_compilation_time: '2025-01-25T14:27:04.799535'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "gcs_bucket", "type":
      "String"}, {"name": "namespace", "type": "String"}, {"name": "training_job_name",
      "type": "String"}, {"name": "training_job_image", "type": "String"}, {"name":
      "learning_rate", "type": "Float"}, {"name": "num_epoch", "type": "Integer"},
      {"default": "", "name": "pipeline-root"}, {"default": "pipeline/mnist-training-pipeline",
      "name": "pipeline-name"}], "name": "mnist-training-pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
spec:
  entrypoint: mnist-training-pipeline
  templates:
  - name: load-and-split-data
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'tensorflow==2.15.0' 'tensorflow-datasets' 'kfp==1.8.22' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def load_and_split_data(train_dataset: Output[Dataset], test_dataset: Output[Dataset]) -> NamedTuple('outputs', train_size=int, test_size=int):
            import tensorflow_datasets as tfds

            (ds_train, ds_test), ds_info = tfds.load(
            'mnist',
            split=['train', 'test'],
            shuffle_files=True,
            as_supervised=True,
            with_info=True,
            )

            ds_train.save(train_dataset.path)
            ds_test.save(test_dataset.path)

            return NamedTuple('outputs', train_size=int, test_size=int)(ds_info.splits['train'].num_examples, ds_info.splits['test'].num_examples)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - load_and_split_data
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, load-and-split-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.11'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {}, "outputParameters": {"test_size": {"type": "INT", "path": "/tmp/outputs/test_size/data"},
          "train_size": {"type": "INT", "path": "/tmp/outputs/train_size/data"}},
          "outputArtifacts": {"test_dataset": {"schemaTitle": "system.Dataset", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/test_dataset/data"},
          "train_dataset": {"schemaTitle": "system.Dataset", "instanceSchema": "",
          "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/train_dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.11
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      parameters:
      - name: load-and-split-data-test_size
        valueFrom: {path: /tmp/outputs/test_size/data}
      - name: load-and-split-data-train_size
        valueFrom: {path: /tmp/outputs/train_size/data}
      artifacts:
      - {name: load-and-split-data-test_dataset, path: /tmp/outputs/test_dataset/data}
      - {name: load-and-split-data-test_size, path: /tmp/outputs/test_size/data}
      - {name: load-and-split-data-train_dataset, path: /tmp/outputs/train_dataset/data}
      - {name: load-and-split-data-train_size, path: /tmp/outputs/train_size/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: mnist-training-pipeline
    inputs:
      parameters:
      - {name: gcs_bucket}
      - {name: pipeline-name}
      - {name: pipeline-root}
    dag:
      tasks:
      - name: load-and-split-data
        template: load-and-split-data
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: preprocess-data
        template: preprocess-data
        dependencies: [load-and-split-data]
        arguments:
          parameters:
          - {name: load-and-split-data-train_size, value: '{{tasks.load-and-split-data.outputs.parameters.load-and-split-data-train_size}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: load-and-split-data-train_dataset, from: '{{tasks.load-and-split-data.outputs.artifacts.load-and-split-data-train_dataset}}'}
      - name: preprocess-data-2
        template: preprocess-data-2
        dependencies: [load-and-split-data]
        arguments:
          parameters:
          - {name: load-and-split-data-test_size, value: '{{tasks.load-and-split-data.outputs.parameters.load-and-split-data-test_size}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: load-and-split-data-test_dataset, from: '{{tasks.load-and-split-data.outputs.artifacts.load-and-split-data-test_dataset}}'}
      - name: tf-minio-to-gcs
        template: tf-minio-to-gcs
        dependencies: [preprocess-data]
        arguments:
          parameters:
          - {name: gcs_bucket, value: '{{inputs.parameters.gcs_bucket}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: preprocess-data-preprocessed_dataset, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-preprocessed_dataset}}'}
  - name: preprocess-data
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'tensorflow==2.15.0' 'kfp==1.8.22' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def preprocess_data(dataset: Input[Dataset], preprocessed_dataset: Output[Dataset], size: int):
            import tensorflow as tf

            ds = tf.data.Dataset.load(dataset.path)
            def normalize_img(image, label):
                """Normalizes images: `uint8` -> `float32`."""
                return tf.cast(image, tf.float32) / 255., label

            ds = ds.map(
            normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
            ds = ds.cache()
            ds = ds.shuffle(size)
            ds = ds.batch(64)
            ds = ds.prefetch(tf.data.AUTOTUNE)
            ds.save(preprocessed_dataset.path)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - preprocess_data
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'size={{inputs.parameters.load-and-split-data-train_size}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.11'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"size": {"type":
          "INT"}}, "inputArtifacts": {"dataset": {"metadataPath": "/tmp/inputs/dataset/data",
          "schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"preprocessed_dataset":
          {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/preprocessed_dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.11
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: load-and-split-data-train_size}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: load-and-split-data-train_dataset, path: /tmp/inputs/dataset/data}
    outputs:
      artifacts:
      - {name: preprocess-data-preprocessed_dataset, path: /tmp/outputs/preprocessed_dataset/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"size": "{{inputs.parameters.load-and-split-data-train_size}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: preprocess-data-2
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'tensorflow==2.15.0' 'kfp==1.8.22' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def preprocess_data(dataset: Input[Dataset], preprocessed_dataset: Output[Dataset], size: int):
            import tensorflow as tf

            ds = tf.data.Dataset.load(dataset.path)
            def normalize_img(image, label):
                """Normalizes images: `uint8` -> `float32`."""
                return tf.cast(image, tf.float32) / 255., label

            ds = ds.map(
            normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
            ds = ds.cache()
            ds = ds.shuffle(size)
            ds = ds.batch(64)
            ds = ds.prefetch(tf.data.AUTOTUNE)
            ds.save(preprocessed_dataset.path)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - preprocess_data
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess-data-2, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'size={{inputs.parameters.load-and-split-data-test_size}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.11'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"size": {"type":
          "INT"}}, "inputArtifacts": {"dataset": {"metadataPath": "/tmp/inputs/dataset/data",
          "schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"preprocessed_dataset":
          {"schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/preprocessed_dataset/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.11
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: load-and-split-data-test_size}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: load-and-split-data-test_dataset, path: /tmp/inputs/dataset/data}
    outputs:
      artifacts:
      - {name: preprocess-data-2-preprocessed_dataset, path: /tmp/outputs/preprocessed_dataset/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"size": "{{inputs.parameters.load-and-split-data-test_size}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: tf-minio-to-gcs
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'tensorflow==2.15.0' 'google-cloud-storage' 'kfp==1.8.22' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def tf_minio_to_gcs(tf_dataset: Input[Dataset], tf_model_artifact: Output[Model], gcs_bucket: str) -> NamedTuple('outputs', dataset_gcs_uri=str,  model_gcs_uri=str):

            import tensorflow as tf
            from google.cloud import storage
            import os
            import glob

            storage_client = storage.Client.create_anonymous_client()
            bucket = storage_client.bucket(gcs_bucket)

            ds = tf.data.Dataset.load(tf_dataset.path)

            for file_path in glob.glob(os.path.join(tf_dataset.path, "**"), recursive=True):
                print("file_path: ", file_path)
                if os.path.isfile(file_path):
                    destination_path = f"{os.path.basename(tf_dataset.path)}/{os.path.basename(file_path)}"

                    blob = bucket.blob(destination_path)
                    blob.upload_from_filename(file_path)
                    print(f"Uploaded {file_path} to {destination_path}")

            dataset_gcs_uri = f"gs://{gcs_bucket}/{os.path.basename(tf_dataset.path)}"
            model_gcs_uri = f"gs://{gcs_bucket}/{os.path.basename(tf_model_artifact.path)}"

            print("Dataset URI: ", dataset_gcs_uri)
            print("Model URI: ", model_gcs_uri)

            return NamedTuple('outputs', dataset_gcs_uri=str, model_gcs_uri=str)(dataset_gcs_uri, model_gcs_uri)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - tf_minio_to_gcs
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, tf-minio-to-gcs, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'gcs_bucket={{inputs.parameters.gcs_bucket}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.11'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"gcs_bucket": {"type":
          "STRING"}}, "inputArtifacts": {"tf_dataset": {"metadataPath": "/tmp/inputs/tf_dataset/data",
          "schemaTitle": "system.Dataset", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {"dataset_gcs_uri": {"type": "STRING", "path":
          "/tmp/outputs/dataset_gcs_uri/data"}, "model_gcs_uri": {"type": "STRING",
          "path": "/tmp/outputs/model_gcs_uri/data"}}, "outputArtifacts": {"tf_model_artifact":
          {"schemaTitle": "system.Model", "instanceSchema": "", "schemaVersion": "0.0.1",
          "metadataPath": "/tmp/outputs/tf_model_artifact/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.11
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: gcs_bucket}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: preprocess-data-preprocessed_dataset, path: /tmp/inputs/tf_dataset/data}
    outputs:
      artifacts:
      - {name: tf-minio-to-gcs-dataset_gcs_uri, path: /tmp/outputs/dataset_gcs_uri/data}
      - {name: tf-minio-to-gcs-model_gcs_uri, path: /tmp/outputs/model_gcs_uri/data}
      - {name: tf-minio-to-gcs-tf_model_artifact, path: /tmp/outputs/tf_model_artifact/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"gcs_bucket": "{{inputs.parameters.gcs_bucket}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "false"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: gcs_bucket}
    - {name: namespace}
    - {name: training_job_name}
    - {name: training_job_image}
    - {name: learning_rate}
    - {name: num_epoch}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/mnist-training-pipeline}
  serviceAccountName: pipeline-runner
